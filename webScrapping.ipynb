{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bf5d28",
   "metadata": {},
   "source": [
    "# Web Scrapping the official AGMARKNET website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9d40fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Install necessary libraries for web scraping and data handling.\n",
    "# `bs4` (BeautifulSoup) is crucial for parsing HTML content. üåê\n",
    "# %pip install -U bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb613807",
   "metadata": {},
   "source": [
    "### üëÄ Manual Data Verification Instructions üìãüîç\n",
    "\n",
    "To manually verify whether onion price data for Uttar Pradesh exists on AGMARKNET: üßÖ\n",
    "\n",
    "1. Visit the official AGMARKNET \"Search Reports\" page: üîó\n",
    " ¬† https://agmarknet.gov.in/SearchCmmMkt.aspx\n",
    "\n",
    "2. In the opened form: üìù\n",
    " ¬† - For 'Commodity', select \"Onion\".\n",
    " ¬† - For 'State', choose \"Uttar Pradesh\".\n",
    " ¬† - (Optional: Select District or Market, or leave as 'All' for a broader search.) üó∫Ô∏è\n",
    " ¬† - Choose the desired Date Range (for best results, use a recent week or month). üóìÔ∏è\n",
    " ¬† - Click the 'Submit' button to fetch results. ‚úÖ\n",
    "\n",
    "3. The page will display a table with Data (Date, Market, Variety, Min Price, Max Price, Modal Price, Arrival Qty) if available. üìä\n",
    "\n",
    "4. If results appear with valid prices and arrival quantities,\n",
    " ¬† ‚úÖ The data exists and can be scraped or downloaded. üéâ\n",
    " ¬† ‚ùå If you see \"No records found\" or empty fields, that period or region has no posted data. üòî\n",
    "\n",
    "5. You can also use the 'Download CSV' button provided on the result page to save a copy for inspection. üíæ\n",
    "\n",
    "‚¨ÜÔ∏è These steps let you confirm that real onion price data for UP exists BEFORE running your code or automating scraping. üöÄ\n",
    "\n",
    "#### üí° Tip: Repeat this for different years or date ranges if you need historical data. üï∞Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b44d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Essential libraries imported successfully!\n",
      "Pandas display options set for a cleaner view of our data. ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# --- Section 1: Setup & Imports üì¶ ---\n",
    "\n",
    "# Let's get our essential tools ready! üõ†Ô∏è\n",
    "# We'll need these Python libraries to make web requests, parse HTML,\n",
    "# and handle data like a pro. üìà\n",
    "\n",
    "import requests                              # For making HTTP requests to websites üåê\n",
    "from bs4 import BeautifulSoup                # For parsing HTML content and navigating the DOM tree üå≥\n",
    "import pandas as pd                          # For powerful data manipulation and analysis with DataFrames üìä\n",
    "import os                                    # For interacting with the operating system (e.g., creating folders) üìÅ\n",
    "import time                                  # For adding delays (important for polite scraping to avoid overwhelming servers! ‚è≥)\n",
    "import urllib.parse                          # For URL encoding/decoding, used in AJAX response parsing and URL construction üîó\n",
    "from datetime import datetime, timedelta     # For handling dates and times precisely üìÖ\n",
    "\n",
    "# Set pandas display options for clearer output in our notebook. ‚ú®\n",
    "pd.set_option('display.max_columns', 100)    # Show up to 100 columns üî¢\n",
    "pd.set_option('display.width', 180)          # Widen the display for better readability of wide tables üìè\n",
    "\n",
    "print(\"üöÄ Essential libraries imported successfully!\")\n",
    "print(\"Pandas display options set for a cleaner view of our data. ‚ú®\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcc93f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Configuration:\n",
      " ¬†Commodity: Onion (Code: 23)\n",
      " ¬†State: Uttar Pradesh (Code: UP)\n",
      " ¬†Date Range: 01-Jul-2025 to 31-Jul-2025\n",
      " ¬†Data will be saved in: data/COMMODITY[23]_UP_01Jul25_31Jul25.csv\n",
      "  AJAX response will be saved in: data/debug_ajax_response_01Jul25_31Jul25.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- ‚öôÔ∏è Configuration & Constants ---\n",
    "# Define the parameters for our data fetching operation. üéØ\n",
    "\n",
    "BASE_URL_MAIN = \"https://agmarknet.gov.in/SearchCmmMkt.aspx\" # The main URL for AGMARKNET price search üåê\n",
    "\n",
    "# Commodity & state codes specific to AGMARKNET portal üßÖüó∫Ô∏è\n",
    "COMMODITY_CODE = \"23\"    # For Onion, 23\n",
    "STATE_CODE = \"UP\"        # For Uttar Pradesh, UP\n",
    "\n",
    "# Set the date range for which we want to fetch the data üìÖ\n",
    "# You can uncomment the lines below to dynamically set the date to \"day before yesterday\".\n",
    "# latest_date = (datetime.today() - timedelta(days=2)).strftime(\"%d-%b-%Y\")\n",
    "# DATE_FROM = latest_date\n",
    "# DATE_TO = latest_date\n",
    "\n",
    "# For now, we'll use a fixed date range for consistency. üóìÔ∏è\n",
    "DATE_FROM = '01-Jul-2025'\n",
    "DATE_TO = '31-Jul-2025'\n",
    "\n",
    "# Define the data directory and the raw CSV file path üìÅüíæ\n",
    "DATA_DIR = \"data\"\n",
    "# Format dates for filename: e.g., '01Jul25_31Jul25'\n",
    "formatted_date_from = DATE_FROM.replace('-', '').replace('20', '')\n",
    "formatted_date_to = DATE_TO.replace('-', '').replace('20', '')\n",
    "RAW_CSV_PATH = os.path.join(DATA_DIR, f\"COMMODITY[{COMMODITY_CODE}]_{STATE_CODE}_{formatted_date_from}_{formatted_date_to}.csv\")\n",
    "AJAX_RESPONSE_PATH = os.path.join(DATA_DIR, f\"debug_ajax_response_{formatted_date_from}_{formatted_date_to}.txt\")\n",
    "\n",
    "# Ensure the data directory exists; create it if it doesn't. ‚ûï\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚öôÔ∏è Configuration:\n",
    " ¬†Commodity: Onion (Code: {COMMODITY_CODE})\n",
    " ¬†State: Uttar Pradesh (Code: {STATE_CODE})\n",
    " ¬†Date Range: {DATE_FROM} to {DATE_TO}\n",
    " ¬†Data will be saved in: {RAW_CSV_PATH}\n",
    "  AJAX response will be saved in: {AJAX_RESPONSE_PATH}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68d2afc1-fa2f-400e-b2c6-2ef7bdea0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- üß© Parse AJAX response function ---\n",
    "# # This function is designed to extract the relevant HTML fragment from the complex ASP.NET AJAX response. üì¶\n",
    "\n",
    "# def parse_ajax_response(ajax_text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Parses ASP.NET pipe-delimited AJAX response to extract and decode the HTML fragment.\n",
    "#     \"\"\"\n",
    "#     parts = ajax_text.split('|')\n",
    "#     i = 0\n",
    "#     while i < len(parts):\n",
    "#         try:\n",
    "#             length = int(parts[i])   # first is length of next block\n",
    "#             update_type = parts[i+1] # usually '#' or 'updatePanel'\n",
    "#             control_id = parts[i+2]\n",
    "#             content = parts[i+3]\n",
    "#             decoded = urllib.parse.unquote(content)\n",
    "#             if \"cphBody_GridPriceData\" in decoded:\n",
    "#                 return decoded\n",
    "#             i += 4\n",
    "#         except Exception:\n",
    "#             i += 1\n",
    "#     print(\"‚ö† Could not parse AJAX properly. Returning raw text.\")\n",
    "#     return ajax_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e269cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ü§ñ Fetch AGMARKNET data function ---\n",
    "# This is the core function for scraping onion price data from AGMARKNET. üßÖüí∞\n",
    "\n",
    "def fetch_agmarknet_data(commodity_code, state_code, date_from, date_to, verbose=True):\n",
    "\n",
    "    session = requests.Session() # Create a session to persist parameters across requests ü§ù\n",
    "    df = pd.DataFrame() # Initialize an empty DataFrame to store results üìù\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nüöÄ Starting data fetch for: {date_from} to {date_to}\") # Inform the user about the process start üöÄ\n",
    "\n",
    "    try:\n",
    "        # Step 1: GET the initial page to extract necessary ASP.NET ViewState and EventValidation tokens. üîë\n",
    "        r = session.get(BASE_URL_MAIN, timeout=15) # Make a GET request with a timeout ‚è∞\n",
    "        r.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx) üö®\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\") # Parse the HTML content üßê\n",
    "\n",
    "        def get_val(name): # Helper function to get input field values üí°\n",
    "            tag = soup.find(\"input\", {\"name\": name}) # Find the input tag by its name attribute üîé\n",
    "            return tag.get(\"value\", \"\") if tag else \"\" # Return its value, or an empty string if not found üìù\n",
    "\n",
    "        viewstate = get_val(\"__VIEWSTATE\") # Extract __VIEWSTATE üîë\n",
    "        viewstategenerator = get_val(\"__VIEWSTATEGENERATOR\") # Extract __VIEWSTATEGENERATOR üîë\n",
    "        eventvalidation = get_val(\"__EVENTVALIDATION\") # Extract __EVENTVALIDATION üîë\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"üìç ViewState found: {bool(viewstate)}, EventValidation found: {bool(eventvalidation)}\") # Report token discovery üéØ\n",
    "\n",
    "        # Step 2: Prepare the payload for the POST request. üì§\n",
    "        # These parameters mimic a form submission on the AGMARKNET website.\n",
    "        payload = {\n",
    "            \"__EVENTTARGET\": \"btnSubmit\",  # Crucial for triggering the form submission üéØ\n",
    "            \"__EVENTARGUMENT\": \"\",         # Usually empty for simple button clicks\n",
    "            \"__VIEWSTATE\": viewstate,      # Required ASP.NET token\n",
    "            \"__VIEWSTATEGENERATOR\": viewstategenerator, # Required ASP.NET token\n",
    "            \"__EVENTVALIDATION\": eventvalidation, # Required ASP.NET token\n",
    "            \"__LASTFOCUS\": \"\",             # Often empty\n",
    "            \"ddlCommodity\": commodity_code, # Our selected commodity (Onion) üßÖ\n",
    "            \"ddlState\": state_code,        # Our selected state (Uttar Pradesh) üó∫Ô∏è\n",
    "            \"txtDate\": date_from,          # Start date for the search üìÖ\n",
    "            \"txtToDate\": date_to,          # End date for the search üìÖ\n",
    "            \"btnSubmit\": \"Submit\"          # The submit button action ‚úÖ\n",
    "        }\n",
    "\n",
    "\n",
    "        # Query parameters for the URL, though the main data fetch is via POST. üåê\n",
    "        query_params = {\n",
    "            \"Tx_Commodity\": commodity_code,\n",
    "            \"Tx_State\": state_code,\n",
    "            \"Tx_District\": \"0\", # \"0\" typically means 'All Districts'\n",
    "            \"Tx_Market\": \"0\",   # \"0\" typically means 'All Markets'\n",
    "            \"DateFrom\": date_from,\n",
    "            \"DateTo\": date_to,\n",
    "            \"Fr_Date\": date_from, # Redundant but included for robustness\n",
    "            \"To_Date\": date_to,\n",
    "            \"Tx_Trend\": \"0\",     # Unused in this context\n",
    "            \"Tx_CommodityHead\": \"Onion\",\n",
    "            \"Tx_StateHead\": \"Uttar Pradesh\",\n",
    "            \"Tx_DistrictHead\": \"--Select--\",\n",
    "            \"Tx_MarketHead\": \"--Select--\"\n",
    "        }\n",
    "\n",
    "        # Construct the full URL for the POST request. üîó\n",
    "        full_post_url = BASE_URL_MAIN + \"?\" + urllib.parse.urlencode(query_params)\n",
    "\n",
    "        # Define HTTP headers to mimic a real browser request. üõ°Ô∏è\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0',\n",
    "            'Referer': BASE_URL_MAIN,\n",
    "            'Origin': 'https://agmarknet.gov.in',\n",
    "            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"üì§ POST URL: {full_post_url}\") # Display the URL for debugging üñ•Ô∏è\n",
    "            print(f\"üîë Payload: {payload}\")      # Display the payload sent üì¶\n",
    "\n",
    "        # Step 3: Send the POST request to get the data. üöÄ\n",
    "        resp = session.post(full_post_url, data=payload, headers=headers, timeout=30) # Send the POST request with data and headers üì®\n",
    "        resp.raise_for_status() # Check for HTTP errors again üö®\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚úÖ AJAX POST status: {resp.status_code}\") # Show response status code üëç\n",
    "            print(f\"üìù Response length: {len(resp.text)}\")     # Show the length of the response text üìè\n",
    "\n",
    "        # Save raw response for inspection during debugging. üíæ\n",
    "        with open(AJAX_RESPONSE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "\n",
    "        # The server didn‚Äôt include the table (cphBody_GridPriceData) in the AJAX response ‚Üí parsing failed ‚Üí no data.\n",
    "        \n",
    "        soup_result = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        table = soup_result.find(\"table\", {\"id\": \"cphBody_GridPriceData\"})\n",
    "        \n",
    "        if table: # If the table is found üéâ\n",
    "            # Extract table headers.\n",
    "            headers = [th.get_text(strip=True) for th in table.find(\"tr\").find_all(\"th\")] # Get column headers üè∑Ô∏è\n",
    "            # Extract table rows (excluding the header row).\n",
    "            rows = [[td.get_text(strip=True) for td in tr.find_all(\"td\")] for tr in table.find_all(\"tr\")[1:]] # Get all data rows üìù\n",
    "            df = pd.DataFrame(rows, columns=headers) # Create a Pandas DataFrame from the extracted data üìä\n",
    "            print(f\"‚úÖ Found table. Rows: {len(df)}\") # Report success and row count üëç\n",
    "        else:\n",
    "            print(\"‚ùå Table not found. Possible reasons: wrong payload, no data, or structure change.\") # Log failure reasons üòî\n",
    "            # Check for \"No records found\" in the *extracted HTML fragment*\n",
    "            if \"No records found\" in html_fragment: # Check for the \"No records found\" message in the parsed HTML ‚ÑπÔ∏è\n",
    "                print(\"‚ÑπÔ∏è Detected: No records found message.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error during data fetching: {e}\") # Catch and report any exceptions during the process ‚ùó\n",
    "    finally:\n",
    "        session.close() # Always close the requests session to release resources. üßπ\n",
    "\n",
    "    return df # Return the DataFrame (might be empty if no data was found or an error occurred) üîÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9288cb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found table. Rows: 4965\n",
      "‚úÖ Data saved to: data/COMMODITY[23]_UP_01Jul25_31Jul25.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sl no.</th>\n",
       "      <th>District Name</th>\n",
       "      <th>Market Name</th>\n",
       "      <th>Commodity</th>\n",
       "      <th>Variety</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Min Price (Rs./Quintal)</th>\n",
       "      <th>Max Price (Rs./Quintal)</th>\n",
       "      <th>Modal Price (Rs./Quintal)</th>\n",
       "      <th>Price Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Auraiya</td>\n",
       "      <td>Achalda</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Red</td>\n",
       "      <td>FAQ</td>\n",
       "      <td>1200</td>\n",
       "      <td>1350</td>\n",
       "      <td>1300</td>\n",
       "      <td>01 Jul 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Auraiya</td>\n",
       "      <td>Achalda</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Red</td>\n",
       "      <td>FAQ</td>\n",
       "      <td>1200</td>\n",
       "      <td>1350</td>\n",
       "      <td>1300</td>\n",
       "      <td>02 Jul 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Auraiya</td>\n",
       "      <td>Achalda</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Red</td>\n",
       "      <td>FAQ</td>\n",
       "      <td>1200</td>\n",
       "      <td>1350</td>\n",
       "      <td>1300</td>\n",
       "      <td>15 Jul 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Auraiya</td>\n",
       "      <td>Achalda</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Red</td>\n",
       "      <td>FAQ</td>\n",
       "      <td>1250</td>\n",
       "      <td>1450</td>\n",
       "      <td>1350</td>\n",
       "      <td>22 Jul 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Auraiya</td>\n",
       "      <td>Achalda</td>\n",
       "      <td>Onion</td>\n",
       "      <td>Red</td>\n",
       "      <td>FAQ</td>\n",
       "      <td>1250</td>\n",
       "      <td>1450</td>\n",
       "      <td>1350</td>\n",
       "      <td>30 Jul 2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sl no. District Name Market Name Commodity Variety Grade Min Price (Rs./Quintal) Max Price (Rs./Quintal) Modal Price (Rs./Quintal)   Price Date\n",
       "0      1       Auraiya     Achalda     Onion     Red   FAQ                    1200                    1350                      1300  01 Jul 2025\n",
       "1      2       Auraiya     Achalda     Onion     Red   FAQ                    1200                    1350                      1300  02 Jul 2025\n",
       "2      3       Auraiya     Achalda     Onion     Red   FAQ                    1200                    1350                      1300  15 Jul 2025\n",
       "3      4       Auraiya     Achalda     Onion     Red   FAQ                    1250                    1450                      1350  22 Jul 2025\n",
       "4      5       Auraiya     Achalda     Onion     Red   FAQ                    1250                    1450                      1350  30 Jul 2025"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- üìù Run scraper & save to CSV ---\n",
    "# Execute the data fetching process and handle the results. üöÄüíæ\n",
    "\n",
    "df_data = fetch_agmarknet_data(\n",
    "    COMMODITY_CODE,        # Pass the onion commodity code üßÖ\n",
    "    STATE_CODE,            # Pass the Uttar Pradesh state code üó∫Ô∏è\n",
    "    DATE_FROM,             # Start date for the data fetch üìÖ\n",
    "    DATE_TO,               # End date for the data fetch üìÖ\n",
    "    False                  # Set verbose to False for cleaner output during execution, True for detailed logs ü§´\n",
    ")\n",
    "\n",
    "if not df_data.empty: # Check if the DataFrame contains any data üì•\n",
    "    df_data.to_csv(RAW_CSV_PATH, index=False, encoding='utf-8') # Save the DataFrame to a CSV file üìÑ\n",
    "    print(f\"‚úÖ Data saved to: {RAW_CSV_PATH}\") # Confirm successful save üéâ\n",
    "    display(df_data.head()) # Display the first few rows of the fetched data for a quick look üëÄ\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data fetched. Please check configuration or website availability.\") # Inform if no data was retrieved üòî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe2921-40c1-4021-a427-4d84e5969911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyazSamachaar venv",
   "language": "python",
   "name": "pyazsamachar-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
